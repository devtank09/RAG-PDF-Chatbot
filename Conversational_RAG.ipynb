{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNEm6fkq7QXUREJupSi8AWP",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/devtank09/RAG-PDF-Chatbot/blob/main/Conversational_RAG.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install langchain langchain-groq faiss-cpu pypdf sentence-transformers python-dotenv langchain-community"
      ],
      "metadata": {
        "id": "5zYNhTZVpr-a"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1SJazvNforRU",
        "outputId": "d4fb5fe6-23ae-4412-a745-f1aa3588d2b2"
      },
      "source": [
        "import os\n",
        "from dotenv import load_dotenv\n",
        "from getpass import getpass\n",
        "\n",
        "from langchain_groq import ChatGroq\n",
        "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
        "from langchain.vectorstores import FAISS\n",
        "from langchain.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
        "from langchain.document_loaders import PyPDFLoader\n",
        "from langchain.chains import create_retrieval_chain, create_history_aware_retriever\n",
        "from langchain.chains.combine_documents import create_stuff_documents_chain\n",
        "from langchain_community.chat_message_histories import ChatMessageHistory\n",
        "from langchain_core.runnables.history import RunnableWithMessageHistory\n",
        "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
        "from langchain_core.documents import Document\n",
        "\n",
        "# Load environment variables from .env file\n",
        "#load_dotenv()\n",
        "\n",
        "def get_groq_api_key():\n",
        "    \"\"\"Gets the Groq API key from environment variables or user input.\"\"\"\n",
        "    api_key = os.environ.get(\"GROQ_API_KEY\")\n",
        "    if not api_key:\n",
        "        print(\"Groq API key not found in environment variables.\")\n",
        "        api_key = getpass(\"Please enter your Groq API Key: \")\n",
        "    return api_key\n",
        "\n",
        "def process_pdf(file_path, embeddings):\n",
        "    \"\"\"Loads, splits, and creates a vector store from a PDF file.\"\"\"\n",
        "    print(f\"Processing PDF: {file_path}\")\n",
        "    try:\n",
        "        loader = PyPDFLoader(file_path)\n",
        "        documents = loader.load()\n",
        "\n",
        "        text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)\n",
        "        splits = text_splitter.split_documents(documents)\n",
        "\n",
        "        if not splits:\n",
        "            print(\"Warning: No text could be extracted from the PDF.\")\n",
        "            return None\n",
        "\n",
        "        print(f\"Type of splits: {type(splits)}\")\n",
        "        if splits:\n",
        "            print(f\"Type of first element in splits: {type(splits[0])}\")\n",
        "            print(f\"First element in splits: {splits[0]}\")\n",
        "\n",
        "        vectorstore = FAISS.from_documents(documents=splits, embedding=embeddings)\n",
        "        print(\"PDF processed successfully. Vector store created.\")\n",
        "        return vectorstore.as_retriever()\n",
        "    except Exception as e:\n",
        "        print(f\"An error occurred while processing the PDF: {e}\")\n",
        "        return None\n",
        "\n",
        "def create_rag_chain(llm, retriever):\n",
        "    \"\"\"Creates the conversational RAG chain.\"\"\"\n",
        "    # 1. Contextualize Question Chain\n",
        "    contextualize_q_system_prompt = (\n",
        "        \"Given a chat history and the latest user question \"\n",
        "        \"which might reference context in the chat history, \"\n",
        "        \"formulate a standalone question which can be understood \"\n",
        "        \"without the chat history. Do NOT answer the question, \"\n",
        "        \"just reformulate it if needed and otherwise return it as is.\"\n",
        "    )\n",
        "    contextualize_q_prompt = ChatPromptTemplate.from_messages(\n",
        "        [\n",
        "            (\"system\", contextualize_q_system_prompt),\n",
        "            MessagesPlaceholder(\"chat_history\"),\n",
        "            (\"human\", \"{input}\"),\n",
        "        ]\n",
        "    )\n",
        "    history_aware_retriever = create_history_aware_retriever(llm, retriever, contextualize_q_prompt)\n",
        "\n",
        "    # 2. Answering Chain\n",
        "    system_prompt = (\n",
        "        \"You are an assistant for question-answering tasks. \"\n",
        "        \"Use the following pieces of retrieved context to answer \"\n",
        "        \"the question. If you don't know the answer, just say that \"\n",
        "        \"you don't know. Keep the answer detailed but relevant.\"\n",
        "        \"\\n\\n\"\n",
        "        \"{context}\"\n",
        "    )\n",
        "    qa_prompt = ChatPromptTemplate.from_messages(\n",
        "        [\n",
        "            (\"system\", system_prompt),\n",
        "            MessagesPlaceholder(\"chat_history\"),\n",
        "            (\"human\", \"{input}\"),\n",
        "        ]\n",
        "    )\n",
        "    question_answer_chain = create_stuff_documents_chain(llm, qa_prompt)\n",
        "\n",
        "    # 3. Combine chains\n",
        "    rag_chain = create_retrieval_chain(history_aware_retriever, question_answer_chain)\n",
        "\n",
        "    return rag_chain\n",
        "\n",
        "def main():\n",
        "    \"\"\"Main function to run the RAG with PDF application.\"\"\"\n",
        "    print(\"--- Welcome to RAG with PDF ---\")\n",
        "\n",
        "    # 1. Get API Key\n",
        "    api_key = get_groq_api_key()\n",
        "    if not api_key:\n",
        "        print(\"Could not get API key. Exiting.\")\n",
        "        return\n",
        "\n",
        "    # 2. Initialize LLM and Embeddings\n",
        "    try:\n",
        "        llm = ChatGroq(groq_api_key=api_key, model_name=\"Gemma2-9b-It\")\n",
        "        embeddings = HuggingFaceEmbeddings(model_name=\"all-MiniLM-L6-v2\")\n",
        "    except Exception as e:\n",
        "        print(f\"Error initializing models: {e}\")\n",
        "        return\n",
        "\n",
        "    # 3. Get PDF and create retriever\n",
        "    retriever = None\n",
        "    while retriever is None:\n",
        "        pdf_path = input(\"Please enter the full path to your PDF file: \").strip()\n",
        "        if os.path.exists(pdf_path) and pdf_path.lower().endswith('.pdf'):\n",
        "            retriever = process_pdf(pdf_path, embeddings)\n",
        "        else:\n",
        "            print(\"Invalid file path or not a PDF. Please try again.\")\n",
        "\n",
        "    # 4. Create the conversational chain\n",
        "    rag_chain = create_rag_chain(llm, retriever)\n",
        "\n",
        "    # 5. Setup chat history\n",
        "    chat_history = ChatMessageHistory()\n",
        "\n",
        "    conversational_rag_chain = RunnableWithMessageHistory(\n",
        "        rag_chain,\n",
        "        lambda session_id: chat_history, # We use the same history object for all sessions\n",
        "        input_messages_key=\"input\",\n",
        "        history_messages_key=\"chat_history\",\n",
        "        output_messages_key=\"answer\",\n",
        "    )\n",
        "\n",
        "    # 6. Start the conversation loop\n",
        "    print(\"\\nPDF loaded. You can now start asking questions.\")\n",
        "    print(\"Type 'exit' or 'quit' to end the chat.\")\n",
        "\n",
        "    while True:\n",
        "        try:\n",
        "            user_input = input(\"\\nYour Question: \")\n",
        "            if user_input.lower() in [\"exit\", \"quit\"]:\n",
        "                print(\"Thank you for using the RAG assistant. Goodbye!\")\n",
        "                break\n",
        "\n",
        "            if not user_input.strip():\n",
        "                continue\n",
        "\n",
        "            print(\"\\nAssistant:\")\n",
        "            # The streaming response\n",
        "            response_stream = conversational_rag_chain.stream(\n",
        "                {\"input\": user_input},\n",
        "                config={\"configurable\": {\"session_id\": \"any_string\"}} # session_id is a placeholder\n",
        "            )\n",
        "\n",
        "            full_response = \"\"\n",
        "            for chunk in response_stream:\n",
        "                if \"answer\" in chunk:\n",
        "                    content = chunk.get(\"answer\", \"\")\n",
        "                    print(content, end=\"\", flush=True)\n",
        "                    full_response += content\n",
        "\n",
        "        except (KeyboardInterrupt, EOFError):\n",
        "            print(\"\\nChat interrupted. Exiting.\")\n",
        "            break\n",
        "        except Exception as e:\n",
        "            print(f\"\\nAn error occurred: {e}\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- Welcome to RAG with PDF ---\n",
            "Groq API key not found in environment variables.\n",
            "Please enter your Groq API Key: ··········\n",
            "Please enter the full path to your PDF file: /content/Explainable machine-learning predictions for catalysts in CO2-assisted propane oxidative dehydrogenation.pdf\n",
            "Processing PDF: /content/Explainable machine-learning predictions for catalysts in CO2-assisted propane oxidative dehydrogenation.pdf\n",
            "Type of splits: <class 'list'>\n",
            "Type of first element in splits: <class 'langchain_core.documents.base.Document'>\n",
            "First element in splits: page_content='Explainable machine-learning predictions for\n",
            "catalysts in CO2-assisted propane oxidative\n",
            "dehydrogenation†\n",
            "Hongyu Liu, ‡ab Kangyu Liu, ‡b Hairuo Zhu, a Weiqing Guoa and Yuming Li*a\n",
            "Propylene is an important raw material in the chemical industry that needs new routes for its production to\n",
            "meet the demand. The CO2-assisted oxidative dehydrogenation of propane (CO2-ODHP) represents an\n",
            "ideal way to produce propylene and uses the greenhouse gas CO2. The design of catalysts with high\n",
            "eﬃciency is crucial in CO2-ODHP research. Data-driven machine learning is currently of great interest\n",
            "and gaining popularity in the heterogeneous catalysis ﬁeld for guiding catalyst development. In this\n",
            "study, the reaction results of CO2-ODHP reported in the literature are combined and analyzed with\n",
            "varied machine learning algorithms such as artiﬁcial neural network (ANN),k-nearest neighbors (KNN),\n",
            "support vector regression (SVR) and random forest regression (RF)and were used to predict the' metadata={'producer': 'PDFium', 'creator': 'PDFium', 'creationdate': 'D:20250724180415', 'moddate': '2025-08-07T16:12:24+05:30', 'source': '/content/Explainable machine-learning predictions for catalysts in CO2-assisted propane oxidative dehydrogenation.pdf', 'total_pages': 7, 'page': 0, 'page_label': '1'}\n",
            "PDF processed successfully. Vector store created.\n",
            "\n",
            "PDF loaded. You can now start asking questions.\n",
            "Type 'exit' or 'quit' to end the chat.\n",
            "\n",
            "Your Question: what are the ML models used\n",
            "\n",
            "Assistant:\n",
            "The machine learning models used are:\n",
            "\n",
            "* **Artificial Neural Network (ANN)**\n",
            "* **K-nearest Neighbors (KNN)**\n",
            "* **Support Vector Regression (SVR)**\n",
            "* **Random Forest Regression (RF)** \n",
            "\n",
            "\n",
            "\n",
            "Your Question: exit\n",
            "Thank you for using the RAG assistant. Goodbye!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "HZ0_7qLLHc14"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}